{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-10T06:08:47.112Z",
     "start_time": "2024-12-10T06:08:44.578829Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import polars as pl\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading data and basic checks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1008c965da207c72"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "shape: (5, 3)\n┌────────┬─────────────────────────────────┬───────┐\n│ ID     ┆ TEXT                            ┆ LABEL │\n│ ---    ┆ ---                             ┆ ---   │\n│ i64    ┆ str                             ┆ i64   │\n╞════════╪═════════════════════════════════╪═══════╡\n│ 614858 ┆ Absolutely excellent. The Gait… ┆ 0     │\n│ 874754 ┆ For these reasons Mr Blifil wa… ┆ 1     │\n│ 895574 ┆ A major factor in the Spanish … ┆ 0     │\n│ 746048 ┆ I joyed also that the old Scri… ┆ 0     │\n│ 205444 ┆ The little tailor went forth, … ┆ 1     │\n└────────┴─────────────────────────────────┴───────┘",
      "text/html": "<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ID</th><th>TEXT</th><th>LABEL</th></tr><tr><td>i64</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>614858</td><td>&quot;Absolutely excellent. The Gait…</td><td>0</td></tr><tr><td>874754</td><td>&quot;For these reasons Mr Blifil wa…</td><td>1</td></tr><tr><td>895574</td><td>&quot;A major factor in the Spanish …</td><td>0</td></tr><tr><td>746048</td><td>&quot;I joyed also that the old Scri…</td><td>0</td></tr><tr><td>205444</td><td>&quot;The little tailor went forth, …</td><td>1</td></tr></tbody></table></div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the dataset\n",
    "train = pl.read_csv(\"./data/improved_train.csv\")\n",
    "val = pl.read_csv(\"./data/improved_val.csv\")\n",
    "\n",
    "train = train[[\"ID\", \"TEXT\", \"LABEL\"]]\n",
    "val = val[[\"ID\", \"TEXT\", \"LABEL\"]]\n",
    "\n",
    "# combining provided training set with the newly assembled one\n",
    "train_provided = pl.read_csv(\"./data/train.csv\")\n",
    "train = pl.concat([train, train_provided])\n",
    "train = train.sample(fraction=1, shuffle=True, seed=894552352)\n",
    "\n",
    "train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T06:08:47.444862Z",
     "start_time": "2024-12-10T06:08:47.159667Z"
    }
   },
   "id": "fa331545fcce0c35"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36798 35532\n"
     ]
    }
   ],
   "source": [
    "# checking distribution of training examples\n",
    "print(\n",
    "    len(train.filter(train[\"LABEL\"] == 0)),\n",
    "    len(train.filter(train[\"LABEL\"] == 1))\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T06:08:47.445781Z",
     "start_time": "2024-12-10T06:08:47.442826Z"
    }
   },
   "id": "2f86af90ce410d0a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "~The data is imbalanced.~ \n",
    "\n",
    "With the newly extracted text, labels are no longer that imbalanced!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6715a95aafc221d6"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# loading RoBERTa\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "\n",
    "# pointing to a custom directory to save the model\n",
    "# initially tried this on xdisk, memory issues, using personal machine\n",
    "custom_cache_dir = \"../.cache_xdisk/\"\n",
    "\n",
    "# loading model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    cache_dir=custom_cache_dir, \n",
    "    trust_remote_code=True,\n",
    "    num_labels=2,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    cache_dir=custom_cache_dir, \n",
    "    trust_remote_code=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T06:08:47.973981Z",
     "start_time": "2024-12-10T06:08:47.446249Z"
    }
   },
   "id": "3feb1d36940bdded"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Absolutely excellent. The Gaither sisters head down south to spend some time with their relatives in Alabama (Big Ma and her mother Ma Charles). While there the girls learn a lot about their ancestry and the feud going on between Ma Charles and her half-sister Miss Trotter. The elderly sisters are storytellers, which really appeals to Vonetta who ends up carrying bickering messages between the two front porches for nearly the entirety of her time down south. When an act of nature sets the whole clan to worrying, family ties from all across the nation end up at Big Ma\\'s. Though there are bound to be questions as to whether this book can truly stand on its own considering the 2 previous books featuring this unforgettable trio of sisters, this book takes a sharp right turn by focusing on the family history. The author gives readers adequate information about characters appearing in previous novels and previous altercations (i. </s>  Weed, the great time-waster, provided a fortuitous exception to the rule, an exception that remains vivid to this day. It was a weekend in the winter of sophomore year. I had gotten stoned, gotten introspective and had wandered off to a lounge on north campus, leaving my partying friends. Not being sleepy, not liking my typically self-critical thoughts, I had prudentially grabbed a book, Collected Poems of Dylan Thomas. Normally, under the influence, reading wasn\\'t easy. Printed words played like eels in shifting, shallow waters. Reading was slow going, distractive, the mind going hither and yon much like the eels. But I had never tried poetry before. To focus, and because his style so obviously demands vocalization, I read the poems aloud, sittling there alone on an ugly couch in an ugly lounge. They were beautiful, impressive. I read a poem, got the concept, the pattern, and read it again, better, with understanding, with proper emphasis. I did my best with what I imagined to be a Welch accent, an imitation of his voice from \"A Child\\'s Christmas in Wales\" which Father had listened to yearly. Certain of the critical faculties being shot, it sounded pretty good. It was quite enjoyable, baroquely enriching. I forgot to be depressed, staying up the night with the music of Dylan Thomas.']\n",
      "['She is a rare artist, this old Mother Nature, who works for the joy of working and not in any spirit of vain show. Today the fir woods are a symphony of greens and greys, so subtle that you cannot tell where one shade begins to be the other. </s> opportunities to extend his knowledge of mankind; for, besides the employment he exercised in public, he was often concerned in private concerts that were given in the hotels of noblemen; by which means he became more and more acquainted with the persons, manners, and characters of high life, which he contemplated with the most industrious attention, as a spectator, who, being altogether unconcerned in the performance, is at more liberty to observe and enjoy the particulars of the entertainment.']\n"
     ]
    }
   ],
   "source": [
    "# replacing [SNIPPET] with separation token for the model\n",
    "train = train.with_columns(\n",
    "    pl.col(\"TEXT\").str.replace(\n",
    "        r\"\\[SNIPPET\\]\", \n",
    "        tokenizer.sep_token\n",
    "    )\n",
    ")\n",
    "\n",
    "val = val.with_columns(\n",
    "    pl.col(\"TEXT\").str.replace(\n",
    "        r\"\\[SNIPPET\\]\", \n",
    "        tokenizer.sep_token\n",
    "    )\n",
    ")\n",
    "\n",
    "print(list(train[0][\"TEXT\"]))\n",
    "print(list(val[0][\"TEXT\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T06:08:48.010010Z",
     "start_time": "2024-12-10T06:08:47.975935Z"
    }
   },
   "id": "a43fab5ed82a6a81"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# tokenizing the dataset\n",
    "\n",
    "train_hf = Dataset.from_polars(train)\n",
    "val_hf = Dataset.from_polars(val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T06:08:48.175993Z",
     "start_time": "2024-12-10T06:08:48.017694Z"
    }
   },
   "id": "f2afeacf0792d693"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def tokenize_function(df):\n",
    "    df_tokenized = tokenizer(df[\"TEXT\"], padding='max_length', truncation=True)\n",
    "    df_tokenized[\"labels\"] = df[\"LABEL\"]\n",
    "    \n",
    "    return df_tokenized"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T06:08:48.179151Z",
     "start_time": "2024-12-10T06:08:48.176328Z"
    }
   },
   "id": "8a499dad34626a93"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/72330 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1fc2d1f93c1344f3ade8bdb7db36e380"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = train_hf.map(tokenize_function, batched=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T06:09:03.179326Z",
     "start_time": "2024-12-10T06:08:48.189607Z"
    }
   },
   "id": "4893ba61d6ae88d3"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/6996 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f01ce833228945dfb3aaaa8b1784048c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_val = val_hf.map(tokenize_function, batched=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T06:09:04.415569Z",
     "start_time": "2024-12-10T06:09:03.180395Z"
    }
   },
   "id": "d0a40f4306a7fe79"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['ID', 'TEXT', 'LABEL', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 72330\n})"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T06:09:04.418945Z",
     "start_time": "2024-12-10T06:09:04.416180Z"
    }
   },
   "id": "24f945d52b22c012"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model fine-tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcdbadba42be28c5"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "[0.9827979781509865, 1.0178149273893955]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since the weight is slightly imbalanced, we will manage this\n",
    "# by informing the optimizer\n",
    "num_pos = len(train.filter(pl.col(\"LABEL\") == 1))\n",
    "num_neg = len(train.filter(pl.col(\"LABEL\") == 0))\n",
    "\n",
    "# finding the inverse frequency\n",
    "neg_weight = len(train) / (2 * num_neg)\n",
    "pos_weight = len(train) / (2 * num_pos)\n",
    "\n",
    "(class_weights := [neg_weight, pos_weight])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T06:09:04.428208Z",
     "start_time": "2024-12-10T06:09:04.419474Z"
    }
   },
   "id": "f16cb67f42647e45"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# slightly modified from https://discuss.huggingface.co/t/how-can-i-use-class-weights-when-training/1067/6\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\") \n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits  \n",
    "        \n",
    "        # moving class weights to the same device as logits\n",
    "        class_weights_tensor = torch.tensor(class_weights).to(logits.device)\n",
    "        \n",
    "        # defining the weighted loss function\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T06:09:04.430575Z",
     "start_time": "2024-12-10T06:09:04.427732Z"
    }
   },
   "id": "511c8c126e3090db"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# defining basic training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_1209/\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=1e-5,\n",
    "    #warmup_steps=500,\n",
    "    #lr_scheduler_type=\"cosine\",\n",
    "    #weight_decay=0.05,\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,            \n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    # metric_for_best_model=\"eval_loss\",\n",
    "    # greater_is_better=False,\n",
    "    logging_dir=f\"./results_1209/logs\",\n",
    "    #fp16=True, # hash this out if on MPS\n",
    "    #ddp_find_unused_parameters=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T06:09:04.460887Z",
     "start_time": "2024-12-10T06:09:04.440397Z"
    }
   },
   "id": "a4977d7f5e6d1056"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='22605' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [    2/22605 : < :, Epoch 0.00/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=22605, training_loss=0.33618110227152403, metrics={'train_runtime': 38166.2049, 'train_samples_per_second': 9.476, 'train_steps_per_second': 0.592, 'total_flos': 9.5154113170944e+16, 'train_loss': 0.33618110227152403, 'epoch': 5.0})"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tuning the model\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T16:45:11.797411Z",
     "start_time": "2024-12-10T06:09:04.443562Z"
    }
   },
   "id": "3d2fc242e640f7d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Actually getting the test labels and compiling .csv for the draft submission "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c916612539f8d5e"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/899 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79aee2befb0e40daba9018665a1c4761"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = pl.read_csv(\"./data/test.csv\")\n",
    "\n",
    "test.with_columns(\n",
    "    pl.col(\"TEXT\").str.replace(\n",
    "        r\"\\[SNIPPET\\]\", \n",
    "        tokenizer.sep_token\n",
    "    )\n",
    ")\n",
    "\n",
    "def tokenize_function_testset(df):\n",
    "    return tokenizer(df[\"TEXT\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "test_hf = Dataset.from_polars(test)\n",
    "tokenized_test = test_hf.map(tokenize_function_testset, batched=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T17:20:39.201522Z",
     "start_time": "2024-12-10T17:20:38.872429Z"
    }
   },
   "id": "cd77ca64591f5216"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the best model\n",
    "best_model = \"./results_1209/checkpoint-22605\"\n",
    "another_attempt = \"./results_new_train/checkpoint-9000\"\n",
    "\n",
    "trained_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    best_model\n",
    ")\n",
    "trained_model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T17:22:13.793956Z",
     "start_time": "2024-12-10T17:22:13.733583Z"
    }
   },
   "id": "9da6a91fd074145c"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# making and retrieving predictions\n",
    "input_ids = tokenized_test[\"input_ids\"]\n",
    "attention_mask = tokenized_test[\"attention_mask\"]\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "# passing tensors to the model\n",
    "with torch.no_grad():\n",
    "    outputs = trained_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    test_labels = torch.argmax(logits, dim=-1).numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T17:23:27.043555Z",
     "start_time": "2024-12-10T17:22:19.744352Z"
    }
   },
   "id": "cb5b9d172388d9e2"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# making and retrieving predictions\n",
    "# predictions_test = trainer.predict(tokenized_test)\n",
    "# logits = predictions_test.predictions\n",
    "# test_labels = logits.argmax(axis=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T17:23:41.346499Z",
     "start_time": "2024-12-10T17:23:41.337833Z"
    }
   },
   "id": "b5abfd9e05d5394"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "pl.DataFrame({\n",
    "    \"ID\": list(test[\"ID\"]),\n",
    "    \"LABEL\": list(test_labels)\n",
    "}).write_csv(\"./results_1209.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T17:23:43.413180Z",
     "start_time": "2024-12-10T17:23:43.406253Z"
    }
   },
   "id": "7e64a6f061596142"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "40e976b3f5b5538b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
